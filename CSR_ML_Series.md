
https://www.coursera.org/learn/advanced-learning-algorithms/lecture/zZ6pa/sampling-with-replacement

https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/RrrOL/what-is-reinforcement-learning

# Random forest algorithm

2026/01/25

0000<->0600

# sampling with replacement

2026/01/25

0000<->0400

# Developing and evaluating an anomaly detection system

2026/01/06

0000<->1100

- unsupervised learning, tune epsilon
- only training and cv set, when anomoly data set is too small, no idea on future performance
- on skewed data, leverage the scores such as precision, recall, F1 scall and so on

# Anomaly detection algorithm

2026/01/05

0000<->1100

# Reducing the number of features (optional)

2025/01/04

0000<->1200

- PCA

# Using per-item features

2025/12/29

0000<->1000

- for different users fitting different linear regression model
- i,j movie i user j
- sum only over the movies of i where it is rated
- cold start problem
- In tree models, particularly in decision trees, we calculate the entropy (or other impurity measures) for each child branch after splitting the data. The goal is to choose the split that results in the lowest entropy,

# The return in reinforcement learning

2025/12/21

0000<->0400

- give less credit, and even less credit
- getting the result sooner would receive better

# Mars rover example

2025/12/21

0000<->0600

- state

# Example of continuous state space applications

2025/12/16

0000<->0200

# Random (stochastic) environment (Optional)

2025/12/16

0000<->0800

- sequence of reward xxxx

# Random (stochastic) environment (Optional)

2025/11/26

0000<->0300

# Bellman Equation

2025/11/26

1000<->1200

# Bellman Equation

2025/11/25

0500<->1000

# Bellman Equation

2025/11/23

0000<->0500

# Bellman Equation

2025/11/14

0000<->0500

# State-action value function definition
2025/11/13

0000<->0500 End

- discount factor and the patient

# State-action value function definition

2025/11/12

0000<->1000 End

- circular definition

# Review of key concepts

2025/11/11

0000<->0500

- policy, take postion as input, give action as output
- MDP, Markov decision process
- future only depends on the current state
- future only depends where you are now
- Q(s,a)
- take option a and behave optimally afterwards

# Review of key concepts

2025/11/10

0000<->0100

# Making decisions: Policies in reinforcement learning

2025/11/10

0000<->0230

- policy
- state, action, return reward, policy

# The Return in reinforcement learning
2025/11/10

0330<->1000

- interest rate, time value of money, vs discount factor

# The Return in reinforcement learning

2025/11/09

0000<->0330

- give less credit to

# Mars rover example

2025/11/09

0000<->0630 End

- reinforcement learning and reward

# What is Reinforcement Learning?

2025/11/07

0000<->0900

- it is impossible to collect enough data for the helicopter problem, that is the major reason that you can’t use supervise learning for that problem
- または　でーた　collection 可能性は　low です

# Week 4
Sampling with replacement

2025/07/19

0000<->0400 End

# Week 4
Using multiple decision trees

2025/07/19

0000<->0400 End

- whisker

# Week 2 SoftMax

2024/12/28

- ezx/ez1+...ezy
- SoftMax regression

# Week 2 Multiclass

2024/12/28

- multiple class classification problem
- RECORD_WEIGHT 167.1
- RECORD_EXERCISE 4 group

# Week 2 Training Details

2024/12/28

# Week 2 TensorFlow implementation

2024/12/28

- loss cost
- meansquareerror for regression problem in NN
- backpropagation is in model.fit

# Week 1 Building a neural network

2024/12/27

# Advanced Learning Algorithms
Week 1 Inference in Code

2024/12/27

- tensorflow
- undercooked, overcooked, too short duration
- tensor, matrix

# Advanced Learning Algorithms Week 1

2024/12/27

- RECORD_WEIGHT 165.7
- inference, download parameters trained by others
