# Gradient Descent for Neural Networks

2024/12/04

0000<->0200

0000<->0800 end

- conversely
- cost function is formed by lost function

# Activation Functions

0000<->1000

2024/11/29

- tanh function
- sigmoid for the output layer, tan h for the hidden layer
- g[1] may be different from g[2]
- rectified linear unit
- leaky ReLu
- ReLu learning is faster, because of derivative
- sigmoid never used 
- tanh is superior than sigmoid
- default ReLu
- optional leaky ReLu

# Activation Functions

0000<->0500

2024/11/28

- sigmoid function vs activation function

# Week 3 Vectorizing Across Multiple Examples

2024/11/25

0400<->0900 End

- stacking up the lower case xs
- activation of the first hidden unit of the training example
- hidden unit num

https://www.coursera.org/learn/neural-networks-deep-learning/home/week/2

# Week 3 Vectorizing Across Multiple Examples

0000<->0300

2024/11/17

# Week 3 Vectorizing Across Multiple Examples

0000<->0500

2024/11/12

# Module 1 more examples of vectrization

2024/11/06

---

# Module 4

# Structuring Machine Learning Projects Week 1 Single Number Evaluation Metric

2024/11/11

0000<->0300

- precision, recall


# Structuring Machine Learning Projects Week 1 Orthogonalization

- training set, dev set, test set
- to relate back to

2024/11/11

# Structuring Machine Learning Projects Week 1 Why ML Strategy

2024/11/11

- collect more diverse training set
