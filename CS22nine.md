# Lecture 1
## REX 1 2022/09/20

2022/09/20

5800<->End

- GPU is one of the decision, more GPU to run code more
- if collecting more data, if using another algorithm
- mlyearing.org
- unsupervised learning application, google news
- 10730 mins, other applications of unsupervised learning
- learn analogy is also a application of unsupervised learning
- enforcement learning 11300 mins

2022/09/14

4025<->5800

- visualize classification problem in 2 D chart ad 1 D chart
- supervised learning, x and y are given to train
- 1 lane road, 2 lane road, intersection


# Lecture 2

2022/10/07

3621<->5121

- batch gradient descent
- when the data set is large, the batch gradient descent would be time consuming
- stochastic gradient descent
- batch is about using all example from the data set just for every one step, and repeatedly
- stochastic gradient never converge, but it is faster
- monitor cost function, which is Jtheta

2022/10/06

2221<->3621

- the theta is the unknown, thus each row from the dataset is the supply that would determines the derivative of the function
- 3217 mins, redo all the diavation, means finishing all the interation of the training examples

2022/10/02

3500<->3900

- theta 0 equal 0, theta 1 equal 0
- iteration by iteration, theta 0 and theta 1 will be changed
- it is different with OLS
- which determines that it is a ML course instead of statistics course
- QA answer why subtract derivative

2022/09/28

2000<->3500

- you have m training examples, so you need to take partial derivative for all those m training examples, theta j = theta j minus simgma of m of partial derivatives
- repeat until convergence
- no local optimal

2022/09/23

1500<->2000

- minimize square difference
- cpls investigation
- 1/2 is to make the math simpler
- gradient descend

2022/09/22

0000<->1500

- input training set output a hypothesis
- how to represent hypothesis
- notations
- how to choose parameter theta
