35 lectures in TTL
### Questions: <br>
PDF PMS CDF, always forgot the difference between them
### Lecture 1 <br>
2022/02/20: finished
### Lecture 2 <br>
2022/02/23: finished
### Lecture 3 <br>
2022/02/24: 1219/4854 birthday problem<br>
2022/03/24: finished, the last problem is hard, I might need to listen the question again, and has something to do with taylor series, 1-1/e
### Lecture 4 <br>
2022/03/26: finished, the begining of this lecture has a continue relation with the end of Lecture 3 <br>

### Lecture 5 <br>
2022/03/30: 1907/5001 law of total probability, conditionging<br>
2022/03/31: finished, conditioning independent doesn't imply unconditioning independent and vice versa
### Lecture 6 <br>

### Lecture 7
2022/03/29: ðŸ’« 2618/5145 , it has someting to do with Lecture 5, law of total probability <br>
2022/03/30: 2618 around, just to review basing on the law of total probility, see what is the new understanding <br>
2022/03/31: finished, excellent course, random variable, is a function which maps result happened in sample space S to X(s), then you got a distribution, the distribution indicates the variness/randomness of the variable, PMF is just like, P(X=3) if you through a coin, probability of number of heads equals to 3 <br>


2022/05/14:

0000<->3000 mins
0100 mins, statistics 110 5 word, conditioning the soul of statistics, r.v. and its distribution;
0913 mins, random walk, absorbing states, TBR eigen value and eiven vector;
1057 mins, recursive property;
1206 mins, condition on something, wish something;
1500 mins, difference equation;
1900 mins, solve differential equation, one method is to guess;
2621 mins, lowbetaw TBR, 0/0;

2022/05/19:
finished,
3000<->End
3306 mins, gambler's ruin, says probability of game going forever, because the result of P(A)+P(B)=1;
3400 mins, r.v.;
3900 mins, r.v. is a function, that map an abstract outcome to a real number;
4500 mins, X=1 is event, the event is {s:X(s)=1};
4754 mins, distribution is blue print of each probability that a r.v. will be a specific number;
### Lecture 8
2022/03/31: 2111/5023 around 14 minutes CDF and PMF, both are a way of describing distribution, CDF P(X<=x), PMF P(X=aj) only applies for discrete problem <br>
2022/03/31: finished, around 2857 minutes by independence, knowing A gives no information about B, vandermonte sigma j=0 to k(m choose k-j times n choose k)=m plus n choose k, 10/100000 will make the difference between with replacement and without replacement sort of the same
### Lecture 9 <br>
2022/03/14: ðŸ’« finished, in order to understand so called taylor Expectation in link [Financial math expectation problems](https://github.com/MediciHouse07/Learning_Records/blob/main/finance_math.md#lecture-3-) <br>
2022/03/31: finished, 2635 minutes Expectation and P has a bridge; 3248 minutes linearity, EX+Y=EX+EY no matter it is dependent or independent; bridge X={1 if A, 0 otherwise, this is indicator R.V. ; 4158 check PMF, geo distribution; 3827 by symatry and bridge

2022/05/24:

0000<->1500 stop at average and mean

0000 mins;

- CDF is a function of real x, not X
- x-> minus infinity, F(x)->0, x -> infinity, F(x)->1
- independence slogan is multiply
- joint probability
- independent of r.v.
- 

1500 mins;

2022/05/25:

1500<->4000

1500 mins

- 2 ways of calculting mean
- fundamental bridge
- unweighted = same weight, weighted = different weight
- E(X) = sigmax x P(X=x) sum over x with P(X=x)>0
- E(X) = 1P(X=1) + 0P(X=0)=P
- indicator r.v.
- k depends on k, n does not depend on k
- QSN why he can re-write the formula, change k=0 to k=1 and why the following equation like that
- because he take one P out

3000 mins

- linearity, true if X and Y are dependent
- bin = np, since X=X1+X2+...+Xn, Xi are ber
- hypergeometric, linearity + indicator + symmetric + fundamental bridge
- 5 card hand, X=#ace, E(X) is the question
- a dependent case of linearity

4500 mins

2022/05/26:

finished,
4000<->End

4000 mins

- geometric distribution is different with hyper geometric
- TBR hyper geometric, just like binomial but not replacement?
- # failures before the first success , each trial is a bern
- Geom(p)
- check if the PMF valid
- X ~ Geom(p), E(X)=q/p
- 

End mins

### Lecture 10 <br>
2022/03/14: ðŸ’« 0610/5010 in order to understand so called taylor Expectation in link [Financial math expectation problems](https://github.com/MediciHouse07/Learning_Records/blob/main/finance_math.md#lecture-3-) <br>
2022/03/31: 2230/5010, 1026 minutes, add 2 functions = compute 2 function and add the result; ends at negative nomial distribution <br>
2022/04/01: finished, round 29 minutes negative binomial distribution, first success distribution; 39 minutes, indication + linearity + symetry, and pubble world
### Lecture 11 <br>
2022/04/01: finished, around 21 minutes, by linearity, binomial converge to poisson, e^x = 1 + e/1 + e^2/2 + ... e^n/n! ; need to check calculus, rate of change, law of lopital <br>
2022/04/26: finished, 0500 mins, random variable - house, distribution - blue print;
0750 mins, poisson, P(X=k), k=0,1,2 lambda>0;
1300 mins, poisson like binomial, number of success;
2200 mins, poisson can have different Pj, events can be dependent;
2900 mins, e^x = (1+x/n)^n, compound annual ?interest?;
3700 mins, Iijk indicator R.V.
### Lecture 12 <br>
2022/04/02: finished, around 1616 minutes FTC from calculus; 3803 minutes, take a function of R.V. is a R.V. ; law of the unconsicious stastician, LOTUS E(g(x))=accumulate from -infinite to +infinite g(x)fX(x)dx; uniform can be everything, just like a indicator I, 4907 minutes, P(F-1(u)<=x)=P(u<=F(x)) because CDF is continously increasing <br>
2022/04/27: finished, 0300 mins, fX(x) specific for PDF, P(X=x) is PMF, f(x) is not probability but it is PDF, intergral is probability;
1500 mins, FTC;
1839 mins, assume derivative exist, for CDF;
2039 mins, expectation is one number summary, variance is spread, absolute value is not differeantiable;
3216 mins, uniform probility propotion to length, completely random;
3924 mins, LOTUS first show;
4423 mins, unif(0,1) can generate all distribution;
4800 mins, proof of uniform is universal, condition of inverse, 1. strictly increase, 2. continuous, to prove this THM you need to know what is CDF, P(X<=x)=F(X)
### Lecture 13 <br>
2022/04/02: finished, 0039 minutes court UK, can't use bayes to defends; 0701 take a function of R.V. is still R.V.; 1200 minutes, simulation; 1455 minutes, linear uniform -> uniform, non-linear uniform -> non-uniform; 1640 independent R.V. and independent event; 3508 double intergral; 3743 handout, jacobian, math about integration <br>
2022/05/02: finished, 0300 mins, universality of universe, CDF, right continus, <- = 0, -> = 1, strictly increase;
1-1/e^x, exponential distribution;
1339 mins, 1-unif ~ unif, symmetry of unnif, a+bu is unif on some interval;
1800 mins, definition of independent, TBR the first time independent is mentioned;
participating;
3500 mins, merge 2 intergral to a double intergral, because when you do it one by one, you will treat one as constant;
3700 mins, Jacobian, TBR, in math handout;
2100 mins, case of pairwise independent <> independent;
3100 mins, taylor series and e^x, antiderivative, porlar coordinate;
3900 mins, intergral e^-u du = 1;
4800 mins, integral by part
### Lecture 14 <br>
2022/04/02: finished, 131 minutes 1st moment, 2nd moment; ðŸŒŸ idea mind flow, key ord and their reason; 1030 minutes, var; 1856 minutes, sum of iid, normal X1-X2~(u1-u2, sigma1^2+sigma2^2); 4458 minutes, prove LOTUS

2022/05/09:
0000 mins,
Normal, E(Z)=0, VAR(Z)=E(Z^2)=1, E(Z^3)=0
0417 mins, -Z is a normal by symmetry;
0522 mins, mean location, SD scale;
0706 mins, VAR(miu + sigma z);
0922 mins, P(X=1)=1 VAR(X)=0;
1037 mins, VAR(X+Y)=VAR(X)+VAR(Y) if X and Y are indepenent, otherwise it doesn't hold;
1230 mins, if you add X+X, they are the same, it will magnify the variability;
1502 mins, derive PDF and CDF of a normal that is with offset location and magifiny of sigma
1859 mins, add 2 normal, the distribution is still a normal with miu1+miu2, sigma1^2 + sigma2^2
X1-X2 has exactly the same property;
Not only they have the expected miu and sigma, they are also normal;
68-95-99.7 rule;
2400 mins, another way of writing PMF;
USF, taylor series, euler formula, put it somewhere so taht you can read;
3138 mins, possion has mean lambda, and variance lambda;
geometric dist is an analogous of exponential dist;
3726 mins, symmetry, indicator r.v. , linearity;
3840 mins, product of indicator r.v. is an indicator r.v.
4015 mins, expect I1 and I2 happens, it is just p1*p2;
4109 mins, hyper geo metric, not independent;
4344 mins, proof of LOTUS TBRT;
4458 mins, r.v. is a function that map sample to x;


### Lecture 16 <br>
2022/04/02: ðŸ’« finished, 1144 memoriless propoerty, exponential is the only memoriless distribution; 1704 mins E(X|x>a) = a+E(X-a|x>a) = a + 1/lambda

2022/05/08: 
0200 mins, Exponential distribution, TBR hyper geometric distribution; TBR MGF
0530 mins, Exp(1), Y=lambdaX;
0809 mins, intergration by part;
1132 mins, exponential distribution, memoriless property; binomial waiting in the discrete time, exponential waiting in the continuous time;
1446 mins, survival function, P(X>=s);
1640 mins, exponential prob is the only one memoriless in continuous , geometric is the only memoriless in discrete;

### Lecture 17 <br>
#### REX 1 2022/04/02
2022/03/14: ðŸ’« MGF [Financial math expectation problems](https://github.com/MediciHouse07/Learning_Records/blob/main/finance_math.md#lecture-3-) <br>
2022/04/02: 3720/5044, 1:00 mins, conditional probability; 910 mins, prove the uniqueness of exp memoryless property; 2238 finite sum is applicable for linearity, infinite sum need to take course real analysis; 2430 mins, interpretation of MGF; 2630 mins, MGF determines distribution; 3018 mins, MGF of X+Y, make X+Y to be easier to be detected; 3500 mins, normal MGF; M^(n)(0)=E(X^n) ; R.V. X has MGF M(t) = E(e^tx)

2022/05/08:
0000<->3000
0129 mins: conditional probability is probability, conditiaonal expectation is expectation;
0247 mins: bias answer;
0455 mins, memoriless property will say, average is 80, when you are 20, then your new E age will be 100
E(T|T>20)=20+E(T), otherwise E(T|T>20) > E(T)
0740 mins, cubic of exponential is a viable
0850 mins, memoriless property is a property of distribution not for a r.v.
0910 mins, if X is a continuous memoriless, then it is a X~Exp(lambda);
I can call myself as librian
1036 mins, memoriless property is G(s+t) = G(s)G(t);
1521 mins, continuous function can swap limit and function, after you take limit, you can make m/n from rational to real number
1800 mins, definition of MGF;
1820 mins, MGF is finite on some interval around 0 otherwise it is not useful, it must be exist to be useful;
2238 mins, if a series is finite, linearity work immediately;
2552 mins, M^n(0)=E(X^n);
2635 mins, MGF determines distribution;
2744 mins, MGF make sum of independent r.v. easy, TBR convolution, TBR C(9,10)=0;

2022/07/19:

0000<->1648

- geommetric and exponential,  both memoriless distribution in discrete world and continuous world
- aha moment is a memoriless case
- proof of exponential is the only memoriless in continuous case
- TBRT
- G(m/n t)
- G is continuous so you can swap and function G

2022/07/20:

1648<->3148

- 1730 mins moment generating function
- M(t)=E(etX)
- taylor series, take derivattive and set it as 0, cef of t^n/nfactorial
- SIGMAx^nt^n/nfactorial
- real analysis, and infinite sum
- MGF determines distribution
- independent, you can split the product
- bernp, MGF
- and thus binomial

### Lecture 18 <br>
2022/03/14: ðŸ’« MGF [Financial math expectation problems](https://github.com/MediciHouse07/Learning_Records/blob/main/finance_math.md#lecture-3-) <br>
2022/04/03: finished, 138 mins moment of inertia; 826 mins benefit of using MGF; 1104 E(Z^n)=0 odd by symetry, normal distribution; 1350 mins geo series converges t<1, taylor series converges everywhere; 2129 mins convolution E(X+Y), take MGF of X * MGF of Y, the result is sum of pois is a pois if they are independent; 2907 mins joint distribution, condition for PMF joint distribution; 4230 mins simple extrem case, analysis method <br>
### Lecture 19 <br>
2022/04/04: finished, 
913 mins, conditional PDF;<br>
347 mins, E g(x,y) by LOTUS;<br>
4100 mins, checken egg problem;<br>
4442 application of law of total probablity;<br>
4600 mins, remove redundancy<br>
### Lecture 20 <br>
2022/04/05: finished,
0000 mins, Ex distance between 2 uniform;
0055 mins, Ex distance between 2 iid standard normal;
independent joint PDF of normal;
0333 mins, MGF applicatioin;
"Know more people", cyber way of interpretation, I got what they say, I store it, I have a map;
0730 mins, easy function;
1600 mins, multinomial;
2150 mins, lumping property;
2600 mins, conditional probability;
3330 mins, cauchy distribution;
3400 mins, symmetry;
4407 mins, u substitution;
4701 mins, LOTP;
4756 mins, independent integration CDF;
### Lecture 21 <br>
#### REX 2 2022/06/14
#### REX 1 2022/04/05
2022/04/05: finished,
2640 mins, dimensionless, unitless;
2959 mins, WLOG, without lose of generality; corr(x,y)=rou;
4200 mins, IA indicator R.V. of event A, IA^2=IA, IA^3=IA, IA*IB=I(A intersect B);
4400 mins, uncorrelated doesn't imply independent;
4800 mins, geometric distribution, use COV formula to prove VAR(X) for geometric distribution

2022/06/10:

0000<->1500

- study 2 RV, is COV
- x,y dependent pair, thus we define COV
- if X=Y then COV formula is variance
- if independent E(XY)=E(X)E(Y)
- use linearity you can rewrite covariance formula to another form
- COV of r.v. and c is 0
- COV(X,Y+Z) use formula and linearity
- bi linearity, treat one variable as constant, then it looks like a linearity for another variable
- it also looks like a distributive property
- play with COV(X+Y,Z+W)=COV(X,Z)+COV(X,W)+COV(Y,Z)+COV(Y,W)
- and a sigma form practice

2022/06/11

1500<->3000

- VAR(X1+X2)
- X1 X2 independent, COV is 0
- VAR(X1+X2+...+Xn), practice to write and arrange i and j and sigma
- X, Y indep, they are uncorrelated COV(X,Y)=0
- converse is false
- odd moment of standard normal with 0 expectation
- Y is a function of X, so Y is dependent of X, X also is a function of Y, at least Y partially determines X
- definition of correlation
- rewrite the function
- WLOG, without lose of generality

2022/06/12

finished,
3000<->End

-  VAR(X+Y) VAR(X-Y)
- proof of correlation is between -1 and 1
- COV i a multinomial, multi dimension of binomial
- competing game
- indicator r.v. to prove the var of binomial
- IAIB = ? intersection
- prove bernuli firstm ten prove binomial
- indicator r.v. practice
- hyper geometric proof

### Lecture 22 <br>
2022/04/06: finished, 
100 mins, hyper geom(w,b,n), withoug replacement binomial;
540 mins, finite population correction;
1101 mins, transformation;
1705 mins, log normal;
2355 mins, jacobian;
2600 mins, convolution - fancy name of sum;
4000 mins +, shenon information theory;<br>

2022/05/05: finished,
0038 mins, hyper geometric;
0220 mins, VAR and COV, expansion of VAR(SIGMA Xi);
0535 mins, finite population correction;
0736 mins, transformation, fY(y) = fX(x)dx/dy, y = g(x), x = g-1(y);
1648 mins, log Normal, don't seperate dx/dy unless you interprete it right;
2115 mins, random vector, n dimension transformation;
2345 mins, Jacobian, TBR, matrix of all possible partial derivative, reciprocal;
2746 mins, convolution, fancy word of sum of random variable;
3212 mins, you can swap derivative and intergral, there is a theory proves that;
3357 mins, probability can be used to prove existence of something that you can't exibht? TBRT (to be reviewed, this);
3916 mins, shennon theory, TBRT;
### Lecture 23 <br>
2022/04/07: finished, 0830 mins conjugate prior;
1049 mins, bayesian statistics;
1620 mins, laplace therom;
2300 mins, doing calculus without calculus;
2602 mins; finance STAT123 course, the same as the teacher in MIT finance math course;
3320 mins, F(R.V.) is a R.V.;
4400 mins, TARP;
4200 mins, FX problem puzzle
2022/04/24: finished, 0225 mins, generalized uniform is beta, bounded by 0 and 1, not flat, beta(a,b), a>0,b>0;
0728 mins, conjugate prior to binomial;
1053 mins, X is observed data, X|P ~ Bin(n,p), P~B(a,b) - prior;
laplace?uniform?;
f=PDF;
1604 mins, interesting explaination of beta;
1741 mins, 1st moment beta, 2nd moment still beta;
nomalizing constant;
2023 mins, bayes billiards;
2415 mins, PDF of uniform is 1;
4420 mins, warrant = call option;
4725 mins, strike price;
4825 mins, LOTUS;

2022/06/01:

0000<->1628

0000 mins

- beta distribution
- integral get beta function
- conjugate prior to binomial
- prior, posterior
- TBR E calculation and COV calculation
- propotionality, why propotionality can work, OK, because constant is just to make integration of PDF to be 1
- 

1500 mins

2022/06/02:

1500<->3000

1500 mins

- story proof

3000 mins

2022/06/03:

finished,
3000<->End

3000 mins

- g(ST), ST is a stock of a company, g is a derivative
- think probability and distribution, then you will think about how much you will pay in the end E(g(ST))
- binormial, bi-variate normal
- warrants = call option, is a derivative
- blackshoes formula

End mins

### Lecture 24 <br>
#### REX 3 2022/07/09
#### REX 2 2022/04/25
#### REX 1 2022/04/08
2022/04/08: finished, 0530 mins, sterling formula;
1133 mins, Gama function factorial formula;
1733 mins, normalizing constant of normal;
3600 mins, MGF proff gama distribution;
2150 mins, transformatijon Y=X/lambda;

2022/04/25: finished, 0600 mins, sterling formula;
0818 mins, gamma function is important;
1305 mins, intergral from 0 to infinity 1/e^x dx = 1;
1420 mins, intergration by parts;
2000 mins, Gamma(a,1), Gamma(a,lambda);
2130 mins, transformation;
2300 mins, Gamma related with many distribution;
2800 mins, memoriless property, poisson & exponential;
4256 mins, LOTUS

2022/07/06:

0000<->1500

- arithmatic sequence
- gamma 1/2 is sqrt(pi)
- normalizing normal, also has something to do with sqrt(pi)

2022/07/07:

0000<->1500

- understand the approximation and limitation
- gamma distribution came from game function
- more integration is needed
- the relation between gamma and integration
- 

2022/07/08:

1500<->3000

- normal formula, and the difference between the two
- how to turn a formula to PDF, normalize
- Gamma(a,lamda), Gamma(a,1)
- poisson processes
- disjoint, joint intervals
- 2800 mins, poisson and exponential
- interarrival times are iid expo(lambda)

2022/07/09:

3000<->End

- negative binomial & geometric
- convolution
- list all the distributions
- induction
- MGF
- to understand this part, MGF, convolution, negative binomial, geometric, posson and exponential
- TBRT
- 4700 mins, the property of gamma function
- Gamma(a,lambda), what does the lambda mean

### Lecture 25 <br>
2022/03/29: ðŸ’« 432/4814, suddenly want to back to basic stuffs <br>
2022/04/09: finished, 332 mins, Beta Gamma, need to review previous 2 videos; transformation function;
900 mins, jacobian, take determinant, take absolute value;
1320 mins, independent definition;
2252 mins, independent E(XY)=E(X)E(Y);
2900 mins, order statistics dependent; Fx CDF fx PDF;
4230 mins, relation of order statistics and beta;
4523 mins, E(X) = E(X|A)P(A) + E(E|Ac)P(Ac) and its proof, using LOTP;
4642 mins, envolop paradox, the same as the movie 21 points said;
### Lecture 26 <br>
2022/04/11: finished, 0300 mins LOTP;
2100 mins, partial progress;
2320 mins, iteration of Ew HH;
2700 mins, ted peter donnelly;
3400 mins, g(x)=E(Y|X=x), E(Y|X)=g(X);
3600 mins, eg E(Y|X)is a r.v.
### Lecture 27 <br>
2022/04/13: finished,0120 mins, best minimize mean square error is the purpose of E(X|Y)
0600 mins, E(Y|X=x) is function of x;
0900 mins, E(h(x)Y|X)=h(x)E(Y|X) taking out what's known;
E(Y|X)=E(Y) if X Y are independent
E(E(Y|X)) = E(Y) adam's law;
E((Y-E(Y|X))hx)=0, residual, uncorrelated;
1900 mins, using plain to understand E(Y|X)
2700 mins, re-arrange term and summation;
3300 mins, VAR(X|Y), EVE's Law;
3700 mins, add variance, just like Chi square
### Lecture 28 <br>
2022/03/14: ðŸ’« Che inequality [Financial math expectation problems](https://github.com/MediciHouse07/Learning_Records/blob/main/finance_math.md#lecture-3-) <br>
2022/04/11:  0000<->0830, 0500 mins, category error;
0727 mins, LOTP;
0830 mins, Adam's law; <br>
2022/04/13: finished, 1700 mins, how to argue in a court;
1900 mins, cauchy-schwarz E(XY) analogy to dot product;
2100 mins, 2D LOTUS; Jension inequality
2816 mins, E(g(X))>=g(E(X)) <- EX^2 >= (EX)^2, because VAR can't be negative;
3738 mins, fundamental bridge;
4345 mins, chebyshev; <br>
2022/05/04: finished, 700 mins, EX=sigma E(X|N)P(N);
0830 mins, adam's law E(X)=E(E(X|N)) E(X|N) means treat N as known;
1034 mins, Var(X) = E(VAR(X|N)) + VAR(E(X|N)) Eve's law;
1300 mins, MGF, TBR MGF,  sigma X = PI MGF of each X;
1900 mins, cauchy schwarz inequanlity, intuitively means corr<=1, it shows correlation how it looks like when miu = 0;
when X and Y uncorrelated, E(XY) = E(X)E(Y);
2412 mins, Jension inequality, g is convex E(g(x)) >= g(E(X)), EX^2 >= E(X)^2 intuitively understanding;
3446 mins, Markov inequality, use bridge indicator;
4100 mins, intuition of Markov;
4200 mins, chebyshev
### Lecture 29 <br>
2022/03/14: ðŸ’« Che inequality [Financial math expectation problems](https://github.com/MediciHouse07/Learning_Records/blob/main/finance_math.md#lecture-3-) <br>
2022/04/13: finished, 0312 mins, LOLN;
1800 mins, degenerate distribution;
3300 mins, by definition;
3900 mins, factorial grows too fast;
4100 mins, appromximation;
4330 mins, fundamental theory CDF - CDF = PDF;
4600 mins, n large & p, pois converge to normal

### Lecture 30 <br>
2022/04/14: finished, 0141 mins, chi square n, chisquare 1 -> take 1 N(0,1) is Gamma(1/2,1/2), chi square n is Gamma (n/2,1/2),
0816 mins, Gosset 1908 student-t;
1337 mins, Cauchy and student t;
2500 mins, LOLN;
2905 mins, MVN;
3000 mins, Random vector;
ind->uncorrelated, uncorrelated!=>ind for univariate, while it seems work for multi-variate
COV(X+Y,X-Y)

### Lecture 31 <br>

#### REX 3 2022/06/25
#### REX 2 2022/05/03
#### REX 1 2022/04/15

2022/04/14: 0<->1928, 0430 mins, stochastics process;
0730 mins, 171 class stochastic process;
1928 mins, lost interest for today <br>
2022/04/15: finished , 1920<->End
2230 mins, Markov monte carlo;
2600 mins, Markov story;
4000 mins, stationary;<br>
2022/05/03: finished, 0346 mins, TBR ineuality;
0519 mins, stochastic processes, X0 X1 X2 with some dependence, one step beyond iid;
0732 mins, cont/discrete time, cont/discrete space
1049 mins, Markov property;
1159 mins, past future, conditionally independent, given the present;
1358 mins, homogeneous, qij independent of t;
2333 mins, Markov chain, converge to a distribution you are interested in, this is MCMC Markov chain monte carlo, build your own MC, TBR fourier;
2641 mins, LOLN hold on iid, free will, LOLN hold in Markov;
3300 mins, LOTP;
4000 mins, stationary, steady state, converge, $\vec{s}$$Q$ = $\vec{s}$, also it is eigen value and eigen vector

2022/06/23

0000<->1500

- chronological
- markov chain, example of stochastic processes
- discrete r.v. and discrete space
- think about continuous r.v. and continuous space
- P(xn+1=j|xn=i,xn-1=xi-1...)
- past and future are conditionally independent given the present
- homogeneous, qij this doesn't depend on time, that means there are some qij depend on time

2022/06/24:

1500<->3000

- transition matrix, stationary property
- markov chain to simlulate something
- LOLN holds in Markov chain case as well
- 1 by M matrix is a vector
- the reason that he uses vector times Q is because of the way he define Q, his Q is on row wise not column wise

2022/06/19:

3000<->End

- he also use row vector, 1 by M vector
- Xn and,  s vector PMF at time n
- s is 1 by M matrix, Q is M by M matrix
-  does it exist, is it unique, stationary dist is referring s vector


### Lecture 32 <br>
#### REX 1 2022/04/16 
2022/04/16: finished, 0640 mins, irr irreducible, you can go anywhere from anywhere;
0812 mins, recurrent, start at a state return to a state; otherwise, transient;
1330 mins, absorbing state;
2230 mins, Markov chain, si=1/ri, ri start at i;
2700 mins, prob vector;
2930 mins, reversible markov chain;
4500 mins, proof of chain, qij=1/di

2022/06/21

0000<->1540

- irreducible chain means you can go anywhere from anywhere
- the opposite is reducible
- reducible can be split to irreducible
- state is recurrent, P =1 of returning to that state
- the opposite is transient
- 1300 mins a case of turning a recurrent structure to transient
- 1400 mins a case of recurrent and transient mix, also a visualization of gambler's ruin
- periodic chain
- 1540 mins summary

2022/06/22

1500<->3000

- s vector is stationary with trans matrix Q
- OK, TBR lecture 31
- TBRT why it is unique
- still TBR markov chain
- reversible MC TBRT

### Lecture 33 <br>
2022/04/16: finished, 1700 mins, Markov chain - google;
2000 mins, google story;
3600 mins, teleportation = fancy word of not follow the hyperlink's instruction, google set the threshold as 0.85
### Lecture 34 <br>
2022/04/15: finished, 0730 mins, probability model->data, inference data->parameter;
1940 mins, mostly harmless econometric book
